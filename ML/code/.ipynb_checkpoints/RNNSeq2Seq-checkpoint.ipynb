{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. preprocessing data\n",
    "# 2. build model\n",
    "# 2.1 encoder\n",
    "# 2.2 attention\n",
    "# 2.3 decoder\n",
    "# 3. evaluation\n",
    "# 3.1 give data, return result\n",
    "# 3.2 visualize\n",
    "\n",
    "path = \"../output/demo2.csv\"\n",
    "dir_path = \"../output/\"\n",
    "encoding_units = 1024\n",
    "batch_size = 1\n",
    "classify = {0: 'ok', 1: 'crosstalked'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "tf.Tensor(\n",
      "[[ 20.    100.      0.      1.65    0.525]\n",
      " [ 20.    100.      0.      5.225   0.725]\n",
      " [ 20.    100.      0.      2.05    0.325]\n",
      " [ 20.    100.      0.      2.6     2.6  ]\n",
      " [ 20.    100.      0.      2.6     2.6  ]\n",
      " [ 20.    100.      0.      1.25    0.8  ]\n",
      " [ 20.    100.      0.      4.85    0.775]\n",
      " [ 20.    100.      0.      4.85    0.775]\n",
      " [ 20.    100.      0.      2.3     2.3  ]\n",
      " [ 20.    100.      0.      0.95    1.   ]\n",
      " [ 20.    100.      0.      0.95    1.   ]\n",
      " [ 20.    100.      0.      0.95    1.   ]\n",
      " [ 20.    100.      0.      0.95    1.   ]\n",
      " [ 20.    100.      0.      5.225   0.975]\n",
      " [ 20.    100.      0.      3.25    0.7  ]\n",
      " [ 20.    100.      0.      5.85    1.3  ]\n",
      " [ 20.    100.      0.      4.3     1.5  ]\n",
      " [ 20.    100.      0.      6.55    1.2  ]\n",
      " [ 20.    100.      0.      2.275   0.625]\n",
      " [ 20.    100.      0.      2.275   0.625]\n",
      " [ 20.    100.      0.      6.55    0.7  ]\n",
      " [ 20.    100.      0.      5.85    1.1  ]\n",
      " [ 20.    100.      0.      4.6     1.3  ]\n",
      " [ 20.    100.      0.      2.275   0.975]\n",
      " [ 20.    100.      0.      2.275   0.975]\n",
      " [ 20.    100.      0.      5.225   0.625]\n",
      " [ 20.    100.      0.      4.85    0.675]\n",
      " [ 20.    100.      0.      4.85    0.675]\n",
      " [ 20.    100.      0.      1.25    1.3  ]\n",
      " [ 20.    100.      0.      4.6     1.2  ]\n",
      " [ 20.    100.      0.      2.65    0.775]\n",
      " [ 20.    100.      0.      1.25    1.1  ]\n",
      " [ 20.    100.      0.      5.55    1.2  ]\n",
      " [ 20.    100.      0.      2.65    0.975]\n",
      " [ 20.    100.      0.      2.275   0.725]\n",
      " [ 20.    100.      0.      2.275   0.725]\n",
      " [ 20.    100.      0.      5.225   1.075]\n",
      " [ 20.    100.      0.      5.225   1.075]\n",
      " [ 20.    100.      0.      4.6     0.9  ]\n",
      " [ 20.    100.      0.      3.65    1.2  ]\n",
      " [ 20.    100.      0.      2.275   1.075]\n",
      " [ 20.    100.      0.      3.9     2.4  ]\n",
      " [ 20.    100.      0.      1.875   0.525]\n",
      " [ 20.    100.      0.      2.275   0.825]\n",
      " [ 20.    100.      0.      2.275   0.825]\n",
      " [ 20.    100.      0.      1.25    0.7  ]\n",
      " [ 20.    100.      0.      4.85    0.925]\n",
      " [ 20.    100.      0.      4.6     1.   ]\n",
      " [ 20.    100.      0.      6.25    1.2  ]\n",
      " [ 20.    100.      0.      5.225   0.825]\n",
      " [ 20.    100.      0.      5.225   0.825]\n",
      " [ 20.    100.      0.      3.9     2.2  ]\n",
      " [ 20.    100.      0.      4.85    0.825]\n",
      " [ 20.    100.      0.      2.65    0.675]\n",
      " [ 20.    100.      0.      3.25    1.2  ]\n",
      " [ 20.    100.      0.      4.85    0.575]\n",
      " [ 20.    100.      0.      0.95    0.7  ]\n",
      " [ 20.    100.      0.      4.85    0.975]\n",
      " [ 20.    100.      0.      2.65    0.925]\n",
      " [ 20.    100.      0.      4.6     1.4  ]], shape=(60, 5), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv(path, header=None)\n",
    "b = a.values\n",
    "# print(type(b))\n",
    "c = tf.convert_to_tensor(b[:, :-1])\n",
    "d = tf.convert_to_tensor(b[:, -1:])\n",
    "print(c.shape[0])\n",
    "# print(d)\n",
    "x = tf.fill((3,219), -999)\n",
    "# x = tf.cast(x, dtype=tf.double)\n",
    "# y = tf.concat([c, x], axis = 0)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/memoforward/Documents/Lab/EMCManagement/code/pythonCode/ML/outputmainboard.csv\n",
      "/Users/memoforward/Documents/Lab/EMCManagement/code/pythonCode/ML/outputmt18vddt6472ag-265c4.csv\n",
      "/Users/memoforward/Documents/Lab/EMCManagement/code/pythonCode/ML/outputSampleAD-01.csv\n",
      "/Users/memoforward/Documents/Lab/EMCManagement/code/pythonCode/ML/outputdemodiff.csv\n",
      "/Users/memoforward/Documents/Lab/EMCManagement/code/pythonCode/ML/outputSampleAD-02.csv\n",
      "/Users/memoforward/Documents/Lab/EMCManagement/code/pythonCode/ML/outputdemo.csv\n",
      "/Users/memoforward/Documents/Lab/EMCManagement/code/pythonCode/ML/outputAD8367_VGA.csv\n",
      "/Users/memoforward/Documents/Lab/EMCManagement/code/pythonCode/ML/outputdemo2.csv\n"
     ]
    }
   ],
   "source": [
    "# print(os.path.abspath(dir_path))\n",
    "for filename in os.listdir(dir_path):\n",
    "    abs_dir_path = os.path.abspath(dir_path)\n",
    "    file_path = abs_dir_path + filename\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def prase_data(file_dir):\n",
    "    \"\"\"\n",
    "    @file_dir: 数据所在文件夹\n",
    "    \"\"\"\n",
    "    maxlen = 0\n",
    "    data_list = []\n",
    "    input_tensor = None\n",
    "    output_tensor = None\n",
    "    \n",
    "    # 1. 读取文件夹下所有的文件，并获取最大的PCB序列长度\n",
    "    for filename in os.listdir(dir_path):\n",
    "        abs_dir_path = os.path.abspath(dir_path)\n",
    "        file_path = abs_dir_path + filename\n",
    "        data = pd.read_csv(file_path)\n",
    "        maxlen = max(data.values.shape[0], maxlen)\n",
    "        data_list.append(data)\n",
    "\n",
    "    # 2. 读取数据，并获得训练的tensor，返回input_tensor和output_tensor\n",
    "    for _data in data_list:\n",
    "        # 2.1 读取数据\n",
    "        # TODO: 在这里添加数据的预处理,比如归一化什么的\n",
    "#         print(_data.values.shape[0])\n",
    "        data = tf.convert_to_tensor(_data.values)\n",
    "        # 2.2 拓展PCB序列长度到同一个维度\n",
    "        padding_len = maxlen - data.shape[0]\n",
    "        if padding_len > 0:\n",
    "            padding_value = tf.fill((padding_len, data.shape[1]), -999)\n",
    "            padding_value = tf.cast(padding_value, dtype=tf.double)\n",
    "            data= tf.concat([data, padding_value], axis = 0)\n",
    "            # _input shape: [1, maxlen, embedding_units]\n",
    "            # _output shape: [1, maxlen, 1]\n",
    "            _input = tf.expand_dims(data[:, : -1], axis = 0)\n",
    "            _output = tf.expand_dims(data[:, -1 :], axis = 0)\n",
    "        if input_tensor is not None:\n",
    "            input_tensor = tf.stack(input_tensor, _input, axis = 0)\n",
    "            output_tensor = tf.stack(output_tensor, _output, axis = 0)\n",
    "        else: \n",
    "            input_tensor = _input\n",
    "            output_tensor = _output\n",
    "            \n",
    "    input_train, output_train, input_eval, output_eval = train_test_split(input_tensor, output_tensor, test_size = 0)\n",
    "    \n",
    "    return input_train, output_train, input_eval, output_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 20\n",
    "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "        dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, batch_size, encoding_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs, hidden):\n",
    "        x = inputs\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden: [b, units]\n",
    "        # encoder_outputs: [b, len, units]\n",
    "        # [b, units] -> [b, 1, units]\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "\n",
    "        # [b, len, units] -> [b, len, 1]\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "        # shape: [b, len, 1]\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # [b, len, 1] -> [b, len, output_units]\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "\n",
    "        # [b, len, output_units] -> [b, output_units]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, batch_size, decoding_units, classifies):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(classifies)\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "\n",
    "    def call(self, inputs, hidden, encoding_outputs):\n",
    "        # [b, 1, embedding_units]： single step decoder\n",
    "        x = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "        # context_vector: [b, units]\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            hidden, encoding_outputs\n",
    "        )\n",
    "\n",
    "        # [b, 1, 2*units]\n",
    "        combined_x = tf.concat(\n",
    "            [tf.expand_dims(context_vector, axis=1), x], axis=-1\n",
    "        )\n",
    "\n",
    "        # output shape: [b, 1, decoding_units]\n",
    "        # state shape: [b, decoding_units]\n",
    "        output, state = self.gru(combined_x)\n",
    "\n",
    "        # [b, 1, decoding_units] -> [b, decoding_units]\n",
    "        output = tf.reshape(output, shape=(-1, output.shape[2]))\n",
    "\n",
    "        # [b, classifies]\n",
    "        predictions = self.fc(output)\n",
    "\n",
    "        return predictions, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    # TODO：以下代码用于去除padding，因为每个PCB板SI结构肯定不一样多\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, -1))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, encoding_hidden):\n",
    "    # inp: [b, len, embedding_units]\n",
    "    # targ: [b, len, classifies]\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden)\n",
    "        decoding_hidden = encoding_hidden\n",
    "\n",
    "        # every SI model's prediction should calculate losses\n",
    "        for t in range(0, inp.shape[1] - 1):\n",
    "            # use original inp to predict\n",
    "            decoding_input = tf.expand_dims(inp[:, t], 1)\n",
    "\n",
    "            predictions, decoding_hidden = decoder.call(decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "    batch_loss = loss / int(targ.shape[0])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_data):\n",
    "    result = []\n",
    "    # attention_matrix = np.zeros()\n",
    "    # encoding_hidden = encoder.initialize_hidden_state()\n",
    "    encoding_hidden = tf.random.normal((1, encoding_units))\n",
    "    _inputs = tf.expand_dims(input_data, axis=0)\n",
    "    encoding_outputs, encoding_hidden = encoder(_inputs, encoding_hidden)\n",
    "    print(\"encoding_outputs:\", encoding_outputs.shape)\n",
    "    print(\"encoding_hidden:\", encoding_hidden.shape)\n",
    "    decoding_hidden = encoding_hidden\n",
    "    decoding_input = tf.zeros((1, 1, input_data.shape[1]))\n",
    "    print(\"decoding_input:\", decoding_input.shape)\n",
    "    for t in range(input_data.shape[0]):\n",
    "        pre, decoding_hidden, attention_weights = decoder.call(decoding_input, decoding_hidden, encoding_outputs)\n",
    "        # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        # attention_matrix[t] =  attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(pre[0]).numpy()\n",
    "        result.append(classify.get(predicted_id))\n",
    "        decoding_input = tf.expand_dims(tf.expand_dims(input_data[t], axis=0), axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
